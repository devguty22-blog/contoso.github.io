<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Document</title>
</head>
<body>
  <main>
    <h1>J. Miguel Gutierrez</h1>
    <h1>PI Solution Architect Challengue</h1>
    <hr>
    <ul>
      <li><h2>Data Architecture</h2></li>
      <figure style="text-align: center;">
        <img src="./assets/images/fig_001.png" 
        alt="Texto alternativo" 
        width="90%" h
        eight="50%" 
        style="display: block; margin: auto;"/>
        <figcaption>Fig 1</figcaption>
      </figure>
      <h2>Sources</h2>
        <p>
          Se identifica dos origenes de datos, el primero de una base de datos on-premise SQL server y el segundo
          de un sharepoint.
        </p>
        <p>
          Se desea llevar datos de tablas de SQL server a un data lake en AWS S3 y archivos de sharepoint.
        </p>
        <p>
          Esto es infraestructura del cliente
        </p>
      <h2>Extraction</h2>
        <P>
          Para realizar la extraccion de datos de SQL Server se debe establecer las reglas de firewall con los VPC publicos
          de AWS con la finalizar de poder tener un tunel de conexion a este servidor On-premise.
        </P>
        <p>
          Para realizar la extraccion de archivos de sharepoint se debe establecer los permisos de conexion con el API de
          sharepoint.
        </P>
        <p>
          Esto se realiza en conjunto con el equipo de seguridad y el equipo de desarrollo.
        </p>
      <h2>ETL</h2>
        <p>
          En la primera primera capa llegaran datos
          crudos y en la segunda capa se realizaran las transformaciones necesarias para el modelamiento de datos.
        </p>
        <p>
          En la tercera capa se realizaran las transformaciones necesarias para el analisis de datos, esto en coordinacion
          con el negocio.
        </p>
        <p>
          Esto se hace en coordinacion con los stakeholders
        </p>
      <h2>ML</h2>
        <p>
          Se realizara el entrenamiento, validacion y despliegue de modelos de machine learning.
        </p>
        <p>
          Esto se hace en coordinacion con los stakeholders
        </p>
      <h2>FrontEnd</h2>
        <p>
          Se realizara el desarrollo de dashboards y reportes.
        </p>
        <p>
          Esto se hace en coordinacion con los stakeholders
        </p>
      <h2>CI/CD</h2>
        <p>
          Se realizara el despliegue de la solucion en un ambiente de desarrollo, pruebas y produccion.
        </p>
      <h2>Orchestation</h2>
        <p>
          Se realizaran los cronogramas de ejecucion de los procesos de ETL, ML y FrontEnd.
        </p>
      <li><h2>Roles</h2></li>
        <ul>
          <li>Delivery Manager (01)</li>
          <li>Data Engineer (02)</li>
          <li>ML Engineer (01)</li>
          <li>FrontEnd Engineer (01)(opcional)</li>
        </ul>
      <li><h2>Tiempos e Hitos</h2></li>
      <figure style="text-align: center;">
        <img src="./assets/images/fig_002.png" 
        alt="Texto alternativo" 
        width="100%" h
        eight="90%" 
        style="display: block; margin: auto;"/>
        <figcaption>Fig 2</figcaption>
      </figure>
      <p>
        Los Hitos son definidos con ayuda de la metodologia Scrum para poder tener cada entregable en el tiempo
        que solicita el cliente, esto va en coordinacion de el manager y el cliente.
      </p>
      <li><h2>Justificacion</h2></li>
      <p>
        La arquitectura propuesta proporciona una solución escalable y flexible que permite 
        centralizar los datos de Contoso Analytics, realizar análisis avanzados y generar reportes 
        interactivos para los usuarios finales. La distribución de roles y la estimación de tiempos 
        garantizan el cumplimiento de los plazos establecidos por el cliente, mientras que el uso de 
        servicios en la nube reduce los costos operativos y garantiza la disponibilidad y seguridad de los datos.
      </p>
      <li><h2>Entregables</h2></li>
      <ul>
        <li>Documento detallado de la arquitectura propuesta.</li>
        <li>Código fuente del pipeline de datos y los modelos analíticos desarrollados.</li>
        <li>Plan de instalación de recursos en produccion</li>
      </ul>
    </ul>
    <hr>
    <h2>Notas</h2>
    <ul>
      <li><h3>Criterios Utilizados</h3></li>
      <p>
        Se utilizo Criterios de ciberseguridad, de infraestructura, de desarrollo, de negocio y de stakeholders.
      </p>
      <p>
        Se utilizo criterios de metodologia agil Hibrida (scrum) para la realizacion de la solucion. El delivery 
        Manager ocupa este frente para la coordinacion de los equipos del cliente como del proveedor.
      </p>
      <p>
        Se utiliza el criterio de costos en infraestructura cloud en el tema de ejecucion de cada etapa de la solucion.
        Mediante el uso de servicios serverless en caso el usuario los tenga o mediante el uso de funciones cloud.
      </p>
      <p>
        Se debe considerar una buena cultura labarol para que el equipo trabaje en conjunto con el cliente y por los
        objetivos
      </p>
      <p>
        Se debe considerar criterios de almancenamiento de datos para poder purgar data innecesaria o archivar datos, 
        asi como poner tener un buckup de datos.
      </p>
      <li><h3>Dificultades</h3></li>
      <p>
        Se debe poner testear la solucion con el cliente para una revision 2, ya que esta solucion va ser ajustada
        conociendo la infraestructura del cliente, por ende toma un poco mas de tiempo.
      </p>
      <p>
        Se debe evaluar la volumetria de datos para el uso eficiente de recursos. 
      </p>
    </ul>
  </main>
</body>
</html>